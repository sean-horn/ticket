#!/usr/bin/env ruby
# encoding: utf-8
# Usage: ticket profile
# Summary: Returns req/sec for the given log file
# Provide ticket completions
# main code handling lifted from support-docs repo

$LOAD_PATH.unshift File.join(ENV['_TICKET_ROOT'], 'share/ticket')

require 'clamp'
require 'helpers/ticket_info'
require 'date'
require 'mixlib/shellout'
require 'descriptive-statistics'

class ProfileCommand < Clamp::Command
  include TicketInfo

  LOG_TYPES = %w{ solr expander hab_nginx nginx nginx2 erchef_requests rabbitmq_overview }

  option '--complete', :flag, 'autocomplete output', hidden: true
  option ['-m', '--minute'], :flag, 'show requests/minute'
  option ['-h', '--hour'], :flag, 'show requests/hour'
  option ['-M', '--avg-minute'], :flag, 'show avg requests/second for each minute window'
  option ['-f', '--filter'], 'filter', 'filter results using', attribute_name: :filter
  option ['-j', '--jq-value'], 'jq_value', 'pull values from json data', attribute_name: :jq_value
  option ['-a', '--accumulator'], :flag, 'values accumulate'
  option ['-s', '--summary'], :flag, 'print statistics at the end'
  option ['-t', '--type'], 'log_type', "log type to profile, options: [#{LOG_TYPES.join(', ')}]", attribute_name: :log_type do |s|
    unless complete?
      raise ArgumentError.new("Invalid log type specified '#{s}'") unless s.nil? or LOG_TYPES.include?(s)
    end
    s
  end
  parameter '[FILE]', 'file', :attribute_name => :file

  def execute
    return autocomplete if complete?

    granularity = :second
    units_label = 'r/s'
    if minute?
      granularity = :minute
      units_label = 'r/m'
    elsif hour?
      granularity = :hour
      units_label = 'r/h'
      units = 3600
    elsif avg_minute?
      granularity = :second_avg
      units_label = 'avg r/s'
      units = 60
    end

    # dup this so we don't get wierd side effects later on
    previous_accumulator_value = nil
    log_filter = filter.dup
    requests = Hash.new { |hash, key| hash[key] = 0 }
    # puts log_type.inspect
    case log_type
    when 'solr', 'expander'
      time_match = '^(.+?)\.\d+\s'
      time_divider = '_'
      if log_filter.nil? && log_type == 'expander'
        log_filter = 'indexed'
      end
    when 'erchef_requests'
      time_match = '^(.+?)Z\s*(.+)'
      time_divider = 'T'
    when 'nginx'
      time_match = '\[(.+?)\]\s*(.+)'
      time_divider = ':'
    when 'nginx2'
      time_match = '\[(.+?)\+(.+)\]\s*(.+)'
      time_divider = 'T'
    when 'hab_nginx'
      time_match = '\[(\d{2}\/.+?)\]\s*(.+)'
      time_divider = ':'
    when 'rabbitmq_overview'
      time_match = '\[(.+?)-\d+:\d+\]\s+(.+)'
      time_divider = nil
      if accumulator?
        units_label = 'Î” msgs'
      else
        units_label = 'msgs'
      end
      units_label += '/s' if units > 1
    else
      time_match = '^(.+?)\.\d+\s*(.+)'
      time_divider = '_'
    end

    signal_usage_error "Please specify a file to profile" if file.nil?
    time_regexp = Regexp.new(time_match)
    leftover_regexp = Regexp.new("#{time_match}(.+)")
    File.open(file, 'r') do |fp|
      fp.each do |line|
        next unless log_filter.nil? || line.match?(/#{log_filter}/)
        results = line.match(time_regexp)
        next if results.nil?

        value = if jq_value.nil?
          1
        else
          json = results[2]
          cmd = "echo '#{json}' | jq -r '#{jq_value}'"
          shellout = Mixlib::ShellOut.new(*cmd)
          shellout.run_command
          shellout.stdout.to_i
        end

        value = accumulate(value) if accumulator?
        # puts 'foo' if requests.nil?
        timestamp = date_glob(results[1], time_divider, granularity)
        requests[timestamp] += value
      end
    end

    # if units > 1
    #   requests.update(requests) do |date, requests_per_minute|
    #     (requests_per_minute / units.to_f).round
    #   end
    # end

    print_chart requests, granularity, units_label

    if summary?
      stats = DescriptiveStatistics::Stats.new(requests.values)
      puts '-' * 80
      puts "Min value: #{stats.min}"
      puts "Max value: #{stats.max}"
      puts "Standard Deviation: #{stats.standard_deviation}"
      puts "90th percentile: #{stats.value_from_percentile(90)}"
      puts '-' * 80
    end
  end

  def accumulate(value)
    # save previous value if not already done
    @previous ||= value

    delta = value - @previous
    @previous = value

    delta
  end

  def autocomplete
    lastargs = [ARGV.last, ARGV[-2]]
    if (lastargs.include?('-t') || lastargs.include?('--type')) && !LOG_TYPES.include?(ARGV.last)
      opts = LOG_TYPES.dup
    else
      opts = %w{ --type --filter --summary --accumulator --unit_time --help }
      # opts += Dir.glob('*')
    end
    opts -= ARGV
    print opts.join("\n")
    exit
  end

  private

  def date_glob(date, time_divider, granularity)

    d = time_divider.nil? ? date : date.sub(time_divider, ' ')

    case granularity
    when :hour
      results = d.match(/(.+):\d\d:\d\d/)
      d = results[1] unless results.nil?
      d += ':00'
    when :minute, :second_avg
      results = d.match(/(.+):\d\d/)
      d = results[1] unless results.nil?
    end

    d
  end

  def print_chart(requests, granularity, units_label)
    largest_requests_per_unit = requests.values.max
    timestamps = requests.keys.sort do |a, b|
      begin
        DateTime.parse(a) <=> DateTime.parse(b)
      rescue
        raise "Invalid date: #{a} or #{b}"
      end
    end
    timestamps.each do |timestamp|
      requests_per_unit = requests[timestamp]
      printf "%s %6d #{units_label} [%-80s]\n", timestamp, requests_per_unit,
             '#' * (requests_per_unit.abs * 80 / largest_requests_per_unit)
    end
  end
end

ProfileCommand.run
